{
  
    
        "post0": {
            "title": "Building a custom OCR using pytorch",
            "content": "Building an OCR from scratch . So in this tutorial, I will give you a basic code walkthrough for building a simple OCR. OCR as might know stands for optical character recognition or in layman terms it means text recognition. Text recognition is one of the classic problems in computer vision and is still relevant today. One of the most important applications of text recognition is the digitization of old manuscripts. Physical copies of books and manuscripts are prone to degradations. With time, the printed characters start to fade. On simple way to preserve such documents is to make a digital copy of it and store it in the cloud or local hard drive which would ensure their continuance. Similarly, text recognition can also be used for licence plate recognition and can also be used in forensics in terms of handwriting recognition. . Okay, now that I have given you enough motivation as to why OCR is important, let me show you how you can build one. So first things first, I’ll start with listing down some of the essential packages that you would need to build your first OCR. We will be working with PyTorch as it is one of the most efficient deep learning libraries present. The other packages are as follows: . Pytorch 1.5 | Matplotlib | Tqdm | textdistance | lmdb | . You can install them either via a pip or conda. I will also be providing a requirements.txt file which you can find in my Github repo. Do a simple pip install -r requirements and you are set to go. . Setting up the Data . We will start our project by importing the libraries. But before that we need data. Now, you are free to use any text image data you might like and for that, you might need to build your own data loader. However, in the interest of keeping things simple, we will be using a neat little package called trdg, which is a synthetic image generator for OCR. You can find all the relevant information regarding this package on its github repository. You can generate printed as well as hand-written text images and infuse them with different kinds of noise and degradation. In this project, I have used trdg to generate printed word images of a single font. You can use any font you like. Just download a .ttf file for your font and while generating the word images be sure to specify the -ft parameter as your font file. . You can generate the word images for training using the following commands: . trdg -i words.txt -c 20000 --output_dir data/train -ft your/fontfile . Here, -c refers to the number of word images you want to generate. words.txt file contains our input word vocabulary while --output_dir and -ft refer to the output and font file respectively. You can similarly generate the test word images for evaluating the performance of your OCR. However, ensure that words for training and testing are mutually exclusive to each other. . Okay, now that we have generated the word images, let us display a few images using matlplotlib %# TODO diplay images from folder . Now lets start importing the libraries that we would need to build our OCR . import os import sys import pdb import six import random import lmdb from PIL import Image import numpy as np import math from collections import OrderedDict from itertools import chain import logging import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset from torch.utils.data import sampler import torchvision.transforms as transforms from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR from torch.nn.utils.clip_grad import clip_grad_norm_ from torch.utils.data import random_split from src.utils.utils import AverageMeter, Eval, OCRLabelConverter from src.utils.utils import EarlyStopping, gmkdir from src.optim.optimizer import STLR from src.utils.utils import gaussian from tqdm import * . Next, let us create our data pipe-line. We do this by inheriting the PyTorch Dataset class. The Dataset class has few methods that we need to adhere to like the __len__ and __getitem__ method. The __len__ method returns the number of items in our dataset while __getitem_ returns the data item for the index passed. You can find more information on PyTorch Dataset class on PyTorch’s official documentation page. . You will observe that we first convert each image into grayscale and convert it into a tensor. This is followed by normalizing the images so that our input data lies within a range of [-1, 1]. We pass all such transformations into a list and later call the transforms to compose function provided by PyTorch. The transform Compose function applies each transformation in the pre-defined order. . class SynthDataset(Dataset): def __init__(self, opt): super(SynthDataset, self).__init__() self.path = os.path.join(opt[&#39;path&#39;], opt[&#39;imgdir&#39;]) self.images = os.listdir(self.path) self.nSamples = len(self.images) f = lambda x: os.path.join(self.path, x) self.imagepaths = list(map(f, self.images)) transform_list = [transforms.Grayscale(1), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))] self.transform = transforms.Compose(transform_list) self.collate_fn = SynthCollator() def __len__(self): return self.nSamples def __getitem__(self, index): assert index &lt;= len(self), &#39;index range error&#39; imagepath = self.imagepaths[index] imagefile = os.path.basename(imagepath) img = Image.open(imagepath) if self.transform is not None: img = self.transform(img) item = {&#39;img&#39;: img, &#39;idx&#39;:index} item[&#39;label&#39;] = imagefile.split(&#39;_&#39;)[0] return item . Next, since we are going to train our model using the mini-batch gradient descent, it is essential that each image in the batch is of the same shape and size. For this, we have defined the SynthCollator class which initially finds the image with maximum width in the batch and then proceeds to pad all images to have the same width. . class SynthCollator(object): def __call__(self, batch): width = [item[&#39;img&#39;].shape[2] for item in batch] indexes = [item[&#39;idx&#39;] for item in batch] imgs = torch.ones([len(batch), batch[0][&#39;img&#39;].shape[0], batch[0][&#39;img&#39;].shape[1], max(width)], dtype=torch.float32) for idx, item in enumerate(batch): try: imgs[idx, :, :, 0:item[&#39;img&#39;].shape[2]] = item[&#39;img&#39;] except: print(imgs.shape) item = {&#39;img&#39;: imgs, &#39;idx&#39;:indexes} if &#39;label&#39; in batch[0].keys(): labels = [item[&#39;label&#39;] for item in batch] item[&#39;label&#39;] = labels return item . Defining our Model . Now we proceed to define our model. We use the CNN-LSTM based architecture which was proposed by Shi et.al. in their excellent paper An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition. The authors used it for scene-text recognition and showed via extensive experimentation that they were able to achieve significant gains in accuracy compared to all other existing methods at that time. . . The figure above shows the architecture used in the paper. The authors used a 7 layered Convolution network with BatchNorm and ReLU. This was followed by a stacked RNN network consisting of two Bidirectional LSTM layers. The convolution layers acted as a feature extractor while the LSTMs layers act as sequence classifiers. The LSTM layers output the probability associated with each output class at each time step Further details can be found in their paper and I strongly suggest you go through it for a better understanding. . The below code snippet is taken from this github repository which provides a Pytorch implementation of their code. . class BidirectionalLSTM(nn.Module): def __init__(self, nIn, nHidden, nOut): super(BidirectionalLSTM, self).__init__() self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True) self.embedding = nn.Linear(nHidden * 2, nOut) def forward(self, input): self.rnn.flatten_parameters() recurrent, _ = self.rnn(input) T, b, h = recurrent.size() t_rec = recurrent.view(T * b, h) output = self.embedding(t_rec) # [T * b, nOut] output = output.view(T, b, -1) return output class CRNN(nn.Module): def __init__(self, opt, leakyRelu=False): super(CRNN, self).__init__() assert opt[&#39;imgH&#39;] % 16 == 0, &#39;imgH has to be a multiple of 16&#39; ks = [3, 3, 3, 3, 3, 3, 2] ps = [1, 1, 1, 1, 1, 1, 0] ss = [1, 1, 1, 1, 1, 1, 1] nm = [64, 128, 256, 256, 512, 512, 512] cnn = nn.Sequential() def convRelu(i, batchNormalization=False): nIn = opt[&#39;nChannels&#39;] if i == 0 else nm[i - 1] nOut = nm[i] cnn.add_module(&#39;conv{0}&#39;.format(i), nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i])) if batchNormalization: cnn.add_module(&#39;batchnorm{0}&#39;.format(i), nn.BatchNorm2d(nOut)) if leakyRelu: cnn.add_module(&#39;relu{0}&#39;.format(i), nn.LeakyReLU(0.2, inplace=True)) else: cnn.add_module(&#39;relu{0}&#39;.format(i), nn.ReLU(True)) convRelu(0) cnn.add_module(&#39;pooling{0}&#39;.format(0), nn.MaxPool2d(2, 2)) # 64x16x64 convRelu(1) cnn.add_module(&#39;pooling{0}&#39;.format(1), nn.MaxPool2d(2, 2)) # 128x8x32 convRelu(2, True) convRelu(3) cnn.add_module(&#39;pooling{0}&#39;.format(2), nn.MaxPool2d((2, 2), (2, 1), (0, 1))) # 256x4x16 convRelu(4, True) convRelu(5) cnn.add_module(&#39;pooling{0}&#39;.format(3), nn.MaxPool2d((2, 2), (2, 1), (0, 1))) # 512x2x16 convRelu(6, True) # 512x1x16 self.cnn = cnn self.rnn = nn.Sequential() self.rnn = nn.Sequential( BidirectionalLSTM(opt[&#39;nHidden&#39;]*2, opt[&#39;nHidden&#39;], opt[&#39;nHidden&#39;]), BidirectionalLSTM(opt[&#39;nHidden&#39;], opt[&#39;nHidden&#39;], opt[&#39;nClasses&#39;])) def forward(self, input): # conv features conv = self.cnn(input) b, c, h, w = conv.size() assert h == 1, &quot;the height of conv must be 1&quot; conv = conv.squeeze(2) conv = conv.permute(2, 0, 1) # [w, b, c] # rnn features output = self.rnn(conv) output = output.transpose(1,0) #Tbh to bth return output . The CTC Loss . Okay, now that we have our data and model pipeline ready, it is time to define our loss function which in our case is the CTC loss function. We will be using PyTorch’s excellent CTC implementation. CTC stands for Connectionist Temporal Classification and was proposed by Alex Graves in his paper Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks. . Honestly, the above work has been a gamechanger for many sequences based tasks like speech and text recognition. For all the sequence-based tasks it is important for the input and output labels to be properly aligned. Proper alignment leads to efficient loss computation between the network predictions and expected output. In segmentation based approaches i.e. when the input word or line has been segmented into its constituent characters, there exists a direct one-to-one mapping between the segmented images of characters and the output labels. However, as you might imagine obtaining such segmentations for each character can be a very tedious and time-consuming task. Thus, CTC based transcription layers have become the de-facto choice for OCRs and speech recognition module since it allows loss computation without explicit mapping between the input and output. The CTC layer takes the output from the LSTMs and computes a score with all possible alignments of the target label. The OCR is then trained to predict a sequence which maximizes the sum of all such scores. . If you want more thorough details regarding the CTC layer I would suggest you go through the following blogs and lecture video . CMU Deep Learning Course Lecture 14 | Sequence Labelling with CTC | . class CustomCTCLoss(torch.nn.Module): # T x B x H =&gt; Softmax on dimension 2 def __init__(self, dim=2): super().__init__() self.dim = dim self.ctc_loss = torch.nn.CTCLoss(reduction=&#39;mean&#39;, zero_infinity=True) def forward(self, logits, labels, prediction_sizes, target_sizes): EPS = 1e-7 loss = self.ctc_loss(logits, labels, prediction_sizes, target_sizes) loss = self.sanitize(loss) return self.debug(loss, logits, labels, prediction_sizes, target_sizes) def sanitize(self, loss): EPS = 1e-7 if abs(loss.item() - float(&#39;inf&#39;)) &lt; EPS: return torch.zeros_like(loss) if math.isnan(loss.item()): return torch.zeros_like(loss) return loss def debug(self, loss, logits, labels, prediction_sizes, target_sizes): if math.isnan(loss.item()): print(&quot;Loss:&quot;, loss) print(&quot;logits:&quot;, logits) print(&quot;labels:&quot;, labels) print(&quot;prediction_sizes:&quot;, prediction_sizes) print(&quot;target_sizes:&quot;, target_sizes) raise Exception(&quot;NaN loss obtained. But why?&quot;) return loss . The Training Loop . The above code snippet builds a wrapper around pytorch’s CTC loss function. Basically, what it does is that it computes the loss and passes it through an additional method called debug, which checks for instances when the loss becomes Nan. . Shout out to Jerin Philip for this code. . Now, let us come to the training loop. The below code might look a bit cumbersome but it provides a nice abstraction which is quite intuitive and easy to use. The below code is based on pytorch lighning’s bolier plate template with few modifications of my own. :P . I will give a basic overview of what it does. Feel free to inspect each method using python debugger. So, the OCRTrainer class takes in the training and validation data. It also takes in the loss function, optimizer and the number of epoch it needs to train the model. The train and validation loader method returns the data loader for the train and validation data. the run_batch method does one forward pass for a batch of image-label pairs. It returns the loss as well as the character and word accuracy. Next, we have the step functions which does the backpropagation, calculates the gradient and updates the parameters for each batch of data. Besides we also have the training_end and validation_end methods that calculate the mean loss and accuracy for each batch after the completion of one single epoch . All, the methods defined are quite simple and I hope you will get the hang of it in no time. . class OCRTrainer(object): def __init__(self, opt): super(OCRTrainer, self).__init__() self.data_train = opt[&#39;data_train&#39;] self.data_val = opt[&#39;data_val&#39;] self.model = opt[&#39;model&#39;] self.criterion = opt[&#39;criterion&#39;] self.optimizer = opt[&#39;optimizer&#39;] self.schedule = opt[&#39;schedule&#39;] self.converter = OCRLabelConverter(opt[&#39;alphabet&#39;]) self.evaluator = Eval() print(&#39;Scheduling is {}&#39;.format(self.schedule)) self.scheduler = CosineAnnealingLR(self.optimizer, T_max=opt[&#39;epochs&#39;]) self.batch_size = opt[&#39;batch_size&#39;] self.count = opt[&#39;epoch&#39;] self.epochs = opt[&#39;epochs&#39;] self.cuda = opt[&#39;cuda&#39;] self.collate_fn = opt[&#39;collate_fn&#39;] self.init_meters() def init_meters(self): self.avgTrainLoss = AverageMeter(&quot;Train loss&quot;) self.avgTrainCharAccuracy = AverageMeter(&quot;Train Character Accuracy&quot;) self.avgTrainWordAccuracy = AverageMeter(&quot;Train Word Accuracy&quot;) self.avgValLoss = AverageMeter(&quot;Validation loss&quot;) self.avgValCharAccuracy = AverageMeter(&quot;Validation Character Accuracy&quot;) self.avgValWordAccuracy = AverageMeter(&quot;Validation Word Accuracy&quot;) def forward(self, x): logits = self.model(x) return logits.transpose(1, 0) def loss_fn(self, logits, targets, pred_sizes, target_sizes): loss = self.criterion(logits, targets, pred_sizes, target_sizes) return loss def step(self): self.max_grad_norm = 0.05 clip_grad_norm_(self.model.parameters(), self.max_grad_norm) self.optimizer.step() def schedule_lr(self): if self.schedule: self.scheduler.step() def _run_batch(self, batch, report_accuracy=False, validation=False): input_, targets = batch[&#39;img&#39;], batch[&#39;label&#39;] targets, lengths = self.converter.encode(targets) logits = self.forward(input_) logits = logits.contiguous().cpu() logits = torch.nn.functional.log_softmax(logits, 2) T, B, H = logits.size() pred_sizes = torch.LongTensor([T for i in range(B)]) targets= targets.view(-1).contiguous() loss = self.loss_fn(logits, targets, pred_sizes, lengths) if report_accuracy: probs, preds = logits.max(2) preds = preds.transpose(1, 0).contiguous().view(-1) sim_preds = self.converter.decode(preds.data, pred_sizes.data, raw=False) ca = np.mean((list(map(self.evaluator.char_accuracy, list(zip(sim_preds, batch[&#39;label&#39;])))))) wa = np.mean((list(map(self.evaluator.word_accuracy, list(zip(sim_preds, batch[&#39;label&#39;])))))) return loss, ca, wa def run_epoch(self, validation=False): if not validation: loader = self.train_dataloader() pbar = tqdm(loader, desc=&#39;Epoch: [%d]/[%d] Training&#39;%(self.count, self.epochs), leave=True) self.model.train() else: loader = self.val_dataloader() pbar = tqdm(loader, desc=&#39;Validating&#39;, leave=True) self.model.eval() outputs = [] for batch_nb, batch in enumerate(pbar): if not validation: output = self.training_step(batch) else: output = self.validation_step(batch) pbar.set_postfix(output) outputs.append(output) self.schedule_lr() if not validation: result = self.train_end(outputs) else: result = self.validation_end(outputs) return result def training_step(self, batch): loss, ca, wa = self._run_batch(batch, report_accuracy=True) self.optimizer.zero_grad() loss.backward() self.step() output = OrderedDict({ &#39;loss&#39;: abs(loss.item()), &#39;train_ca&#39;: ca.item(), &#39;train_wa&#39;: wa.item() }) return output def validation_step(self, batch): loss, ca, wa = self._run_batch(batch, report_accuracy=True, validation=True) output = OrderedDict({ &#39;val_loss&#39;: abs(loss.item()), &#39;val_ca&#39;: ca.item(), &#39;val_wa&#39;: wa.item() }) return output def train_dataloader(self): # logging.info(&#39;training data loader called&#39;) loader = torch.utils.data.DataLoader(self.data_train, batch_size=self.batch_size, collate_fn=self.collate_fn, shuffle=True) return loader def val_dataloader(self): # logging.info(&#39;val data loader called&#39;) loader = torch.utils.data.DataLoader(self.data_val, batch_size=self.batch_size, collate_fn=self.collate_fn) return loader def train_end(self, outputs): for output in outputs: self.avgTrainLoss.add(output[&#39;loss&#39;]) self.avgTrainCharAccuracy.add(output[&#39;train_ca&#39;]) self.avgTrainWordAccuracy.add(output[&#39;train_wa&#39;]) train_loss_mean = abs(self.avgTrainLoss.compute()) train_ca_mean = self.avgTrainCharAccuracy.compute() train_wa_mean = self.avgTrainWordAccuracy.compute() result = {&#39;train_loss&#39;: train_loss_mean, &#39;train_ca&#39;: train_ca_mean, &#39;train_wa&#39;: train_wa_mean} # result = {&#39;progress_bar&#39;: tqdm_dict, &#39;log&#39;: tqdm_dict, &#39;val_loss&#39;: train_loss_mean} return result def validation_end(self, outputs): for output in outputs: self.avgValLoss.add(output[&#39;val_loss&#39;]) self.avgValCharAccuracy.add(output[&#39;val_ca&#39;]) self.avgValWordAccuracy.add(output[&#39;val_wa&#39;]) val_loss_mean = abs(self.avgValLoss.compute()) val_ca_mean = self.avgValCharAccuracy.compute() val_wa_mean = self.avgValWordAccuracy.compute() result = {&#39;val_loss&#39;: val_loss_mean, &#39;val_ca&#39;: val_ca_mean, &#39;val_wa&#39;: val_wa_mean} return result . Putting Everything Together . And, finally, we have the Learner class. It implements a couple of more methods like the save and load model. It also tracks the losses and saves them in a csv file. This comes in handy if we want to analyze the behaviour of our training and validation loops. It initializes our OCRTrainer module with the necessary hyperparameters and later calls the fit method which runs the training loop. . Besides these methods, we have a bunch of helper methods like the OCRLabel_converter, Eval and Averagemeter. I am not including them in this notebook, instead, I have written them in utils.py file and I am importing them from there. In case you want to take a peek, feel free to tinker with the utils.py file. All the necessary documentation is provided in the file itself. . class Learner(object): def __init__(self, model, optimizer, savepath=None, resume=False): self.model = model self.optimizer = optimizer self.savepath = os.path.join(savepath, &#39;best.ckpt&#39;) self.cuda = torch.cuda.is_available() self.cuda_count = torch.cuda.device_count() if self.cuda: self.model = self.model.cuda() self.epoch = 0 if self.cuda_count &gt; 1: print(&quot;Let&#39;s use&quot;, torch.cuda.device_count(), &quot;GPUs!&quot;) self.model = nn.DataParallel(self.model) self.best_score = None if resume and os.path.exists(self.savepath): self.checkpoint = torch.load(self.savepath) self.epoch = self.checkpoint[&#39;epoch&#39;] self.best_score=self.checkpoint[&#39;best&#39;] self.load() else: print(&#39;checkpoint does not exist&#39;) def fit(self, opt): opt[&#39;cuda&#39;] = self.cuda opt[&#39;model&#39;] = self.model opt[&#39;optimizer&#39;] = self.optimizer logging.basicConfig(filename=&quot;%s/%s.csv&quot; %(opt[&#39;log_dir&#39;], opt[&#39;name&#39;]), level=logging.INFO) self.saver = EarlyStopping(self.savepath, patience=15, verbose=True, best_score=self.best_score) opt[&#39;epoch&#39;] = self.epoch trainer = OCRTrainer(opt) for epoch in range(opt[&#39;epoch&#39;], opt[&#39;epochs&#39;]): train_result = trainer.run_epoch() val_result = trainer.run_epoch(validation=True) trainer.count = epoch info = &#39;%d, %.6f, %.6f, %.6f, %.6f, %.6f, %.6f&#39;%(epoch, train_result[&#39;train_loss&#39;], val_result[&#39;val_loss&#39;], train_result[&#39;train_ca&#39;], val_result[&#39;val_ca&#39;], train_result[&#39;train_wa&#39;], val_result[&#39;val_wa&#39;]) logging.info(info) self.val_loss = val_result[&#39;val_loss&#39;] print(self.val_loss) if self.savepath: self.save(epoch) if self.saver.early_stop: print(&quot;Early stopping&quot;) break def load(self): print(&#39;Loading checkpoint at {} trained for {} epochs&#39;.format(self.savepath, self.checkpoint[&#39;epoch&#39;])) self.model.load_state_dict(self.checkpoint[&#39;state_dict&#39;]) if &#39;opt_state_dict&#39; in self.checkpoint.keys(): print(&#39;Loading optimizer&#39;) self.optimizer.load_state_dict(self.checkpoint[&#39;opt_state_dict&#39;]) def save(self, epoch): self.saver(self.val_loss, epoch, self.model, self.optimizer) . Defining the hyperparameters . Okay, now that we have set the premise, its time to unfold the drama. We begin by defining our vocabulary i.e. the alphabets which serve as the output classes for our model. We define a suitable name for this experiment which will also serve as the folder name where the checkpoints and log files will be stored. We also define the hyper-parameters like the batch size, learning rate, image height, number of channels etc. . Then we initialize our Dataset class and split the data into train and validation. We then proceed to initialize our Model and CTCLoss and finally call the learner.fit function. . Once the training is over we can find the saved model in the checkpoints/name folder. We may load the model and evaluate its performance on the test data or finetune it on some other data. . alphabet = &quot;&quot;&quot;Only thewigsofrcvdampbkuq.$A-210xT5&#39;MDL,RYHJ&quot;ISPWENj&amp;BC93VGFKz();#:!7U64Q8?+*ZX/%&quot;&quot;&quot; args = { &#39;name&#39;:&#39;exp1&#39;, &#39;path&#39;:&#39;data&#39;, &#39;imgdir&#39;: &#39;train&#39;, &#39;imgH&#39;:32, &#39;nChannels&#39;:1, &#39;nHidden&#39;:256, &#39;nClasses&#39;:len(alphabet), &#39;lr&#39;:0.001, &#39;epochs&#39;:4, &#39;batch_size&#39;:32, &#39;save_dir&#39;:&#39;checkpoints&#39;, &#39;log_dir&#39;:&#39;logs&#39;, &#39;resume&#39;:False, &#39;cuda&#39;:False, &#39;schedule&#39;:False } data = SynthDataset(args) args[&#39;collate_fn&#39;] = SynthCollator() train_split = int(0.8*len(data)) val_split = len(data) - train_split args[&#39;data_train&#39;], args[&#39;data_val&#39;] = random_split(data, (train_split, val_split)) print(&#39;Traininig Data Size:{} nVal Data Size:{}&#39;.format( len(args[&#39;data_train&#39;]), len(args[&#39;data_val&#39;]))) args[&#39;alphabet&#39;] = alphabet model = CRNN(args) args[&#39;criterion&#39;] = CustomCTCLoss() savepath = os.path.join(args[&#39;save_dir&#39;], args[&#39;name&#39;]) gmkdir(savepath) gmkdir(args[&#39;log_dir&#39;]) optimizer = torch.optim.Adam(model.parameters(), lr=args[&#39;lr&#39;]) learner = Learner(model, optimizer, savepath=savepath, resume=args[&#39;resume&#39;]) learner.fit(args) . Evaluation and testing . import matplotlib.pyplot as plt from torchvision.utils import make_grid . device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) def get_accuracy(args): loader = torch.utils.data.DataLoader(args[&#39;data&#39;], batch_size=args[&#39;batch_size&#39;], collate_fn=args[&#39;collate_fn&#39;]) model = args[&#39;model&#39;] model.eval() converter = OCRLabelConverter(args[&#39;alphabet&#39;]) evaluator = Eval() labels, predictions, images = [], [], [] for iteration, batch in enumerate(tqdm(loader)): input_, targets = batch[&#39;img&#39;].to(device), batch[&#39;label&#39;] images.extend(input_.squeeze().detach()) labels.extend(targets) targets, lengths = converter.encode(targets) logits = model(input_).transpose(1, 0) logits = torch.nn.functional.log_softmax(logits, 2) logits = logits.contiguous().cpu() T, B, H = logits.size() pred_sizes = torch.LongTensor([T for i in range(B)]) probs, pos = logits.max(2) pos = pos.transpose(1, 0).contiguous().view(-1) sim_preds = converter.decode(pos.data, pred_sizes.data, raw=False) predictions.extend(sim_preds) # make_grid(images[:10], nrow=2) fig=plt.figure(figsize=(8, 8)) columns = 4 rows = 5 for i in range(1, columns*rows +1): img = images[i] img = (img - img.min())/(img.max() - img.min()) img = np.array(img * 255.0, dtype=np.uint8) fig.add_subplot(rows, columns, i) plt.title(predictions[i]) plt.axis(&#39;off&#39;) plt.imshow(img) plt.show() ca = np.mean((list(map(evaluator.char_accuracy, list(zip(predictions, labels)))))) wa = np.mean((list(map(evaluator.word_accuracy_line, list(zip(predictions, labels)))))) return ca, wa . args[&#39;imgdir&#39;] = &#39;test&#39; args[&#39;data&#39;] = SynthDataset(args) resume_file = os.path.join(args[&#39;save_dir&#39;], args[&#39;name&#39;], &#39;best.ckpt&#39;) if os.path.isfile(resume_file): print(&#39;Loading model %s&#39;%resume_file) checkpoint = torch.load(resume_file) model.load_state_dict(checkpoint[&#39;state_dict&#39;]) args[&#39;model&#39;] = model ca, wa = get_accuracy(args) print(&quot;Character Accuracy: %.2f nWord Accuracy: %.2f&quot;%(ca, wa)) else: print(&quot;=&gt; no checkpoint found at &#39;{}&#39;&quot;.format(save_file)) print(&#39;Exiting&#39;) . 0%| | 0/2 [00:00&lt;?, ?it/s] Loading model checkpoints/exp1/best.ckpt 100%|██████████| 2/2 [00:00&lt;00:00, 2.25it/s] . . Character Accuracy: 98.89 Word Accuracy: 0.98 . . .",
            "url": "https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html",
            "relUrl": "/markdown/2020/08/29/building-ocr.html",
            "date": " • Aug 29, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "A primer on semi-supervised Learning",
            "content": "Deep Learning (DL) algorithms typically rely on a huge amount of labelled data pairs. However, it is often expensive to collect such annotated datasets in terms of both cost and time. ImageNet, the largest image database in the present day consists of around 14 million images. Each image in it was hand-annotated by several annotators using a crowdsourcing platform known as Amazon Mechanical Turk. There are several other image datasets like PASCAL VOC and MS COCO which consist far fewer images compared to ImageNet (10k and 100k respectively) and it is possible to train a DL network on such dataset satisfactorily to learn a new task. However, the general trend in DL literature suggests that the performance of the models can further be improved if more and more data is added. . Transfer Learning . As mentioned above Deep Learning algorithms are essentially data-hungry methods which require tons of data to estimate its millions of parameters. This property effectively renders it useless to problems that have limited training data. However, DL models have this remarkable ability where the representations learned over large datasets can be effectively transferred to tasks which have a limited amount of training data. [2] explored applied this idea and applied it to train deep CNN models on limited labelled data. The observed that . CNN can act as a generic extractor of mid-level image representation, which can be pre-trained on one dataset (the source task) and then re-used on other target tasks. . The authors pre-trained a network on 1000 classes of ImageNet data and used to perform object detection on Pascal VOC data. Applying this method they were able to achieve a significant gain in performance compared to the existing baseline results. Besides, transfer learning has shown great promise in other computer vision tasks such as segmentation [3] and recognition [4]. . Although, transfer learning is a very useful trick for improving gains on tasks with limited training data, yet it does not take advantage of the massive amounts of unlabeled data available over the internet. Also, it suffers from the in consequence of having to annotate a portion of target data which can prove to be quite a lot if the task involves speech or text recognition. In such a case each word and utterance has to be correctly transcribed manually which can prove to be very tedious and time-consuming. . Semi-supervised Learning . These above drawbacks can be easily overcome by semi-supervised learning algorithms which are designed is such a way that they can work with both labelled and unlabeled data. Consider Google Images or Instagram. A simple query can fetch you thousands of results. However, the retrieved images are unstructured and unannotated and cannot be put to use if we are using supervised learning algorithms. Semi-supervised learning algorithms, on the other hand, make use of not only the labelled target data but also use the myriads of unlabeled data to learn better representations. This property gives an edge to the SSL algorithms over traditional finetuning approaches. There exist mainly two approaches towards implementing SSL. The first approach involves passing of the unlabeled images through different augmentations and perturbations. Since the images are constant, the model predictions should not be swayed by the perturbations and predict the same label. Forcing the model to come up with the same prediction under a different set of noise/perturbation can act as a source of regularization and together with the supervised loss term, it helps the DL model towards more stable generalization during testing and also helps the model learn more robust invariant features [5, 6]. The second approach involves inferring labels for the unlabeled data and which is then ( pseudo-labelled data) used to train the model with a supervised loss term. The second approach which is also known as pseudo-labelling falls under the category of transductive learning where both labelled and unlabeled data is used to improve the performance of a model. Labels for the unlabeled data can be inferred in two ways. 1) By constructing a graph and propagating labels from known to unknown data points and 2) By using an existing pre-trained classifier to invoke the labels unlabeled data points. . Label Propagation . Label propagation [7, 8] deals with the construction of a graph between the labelled and unlabeled data points which is later used to propagate the labels from labelled to unlabeled data using cluster assumptions. Label Propagation comprises of two steps: construction of a graph and inference. In the graph construction stage, data points of both labelled and unlabeled data form the nodes while the edges represent the similarity between the data points. Larger edge weights indicate higher similarity between the data points and vice-versa. The most common technique to create a graph is using a clustering algorithm such as kNN where the edge weights are obtained using as a Euclidean based distance function. In the inference stage, the labels from the labelled data are propagated to their nearest unlabeled data points along with a certainty score which is simply the Euclidean distance from the nearest labelled data point. . Self-training . Self-training [9, 10] was one of the earliest attempts to use unlabeled data to boost model performance. Self-training comprises of two stages: Initially, the model is trained on a limited amount of labelled data using a supervised method. In the next stage, the learned model is used to predict the labels of unlabeled data points. Finally, the model is trained on both the labelled and unlabeled data where the predictions of unlabeled data are treated as target labels. One of the major concerns of self-training is that initial trained model might predict a significant amount of unlabeled data erroneously. This might bring down the performance of a model while training rather than improving. Care should be taken to minimize the number of noisy predictions in the training set. One way to do so would be to screen the predictions effectively and include only those predictions on which the model is highly confident. Despite the screening measure, some noisy predictions still manage to creep into the training data, which hinders the learning of the ML algorithm. Most works in this domain discuss providing perturbations to the input data or model as a way to overcome confirmation bias. We will see in the next few sections of how linear perturbation is known as mixup [11] and as well as some other regularization method are used to improve the performance of a text recognition system on an unlabeled target dataset. . In the next post, we will get our hands dirty trying to implement self-training algorithm on a classic computer vision problem of text-recognition or more commonly known as OCR. . [1] ImageNet Large Scale Visual Recognition Challenge . [2] Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks . [3] Fully Convolutional Networks for Semantic Segmentation . [4] Deep Residual Learning for Image Recognition . [5] Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning] . [6] Temporal Ensembling for semi-supervised learning . [7] Label Propagation for Deep Semi-supervised Learning . [8] Learning by association â€“ a versatile semi-supervised training method for neural networks . [9] Probability of error of some adaptive pattern-recognition machines . [10] Learning to recognize patterns without a teacher . [11] mixup beyond empirical risk minimization .",
            "url": "https://deepayan137.github.io/blog/markdown/2020/08/12/Self-Training.html",
            "relUrl": "/markdown/2020/08/12/Self-Training.html",
            "date": " • Aug 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ".",
          "url": "https://deepayan137.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://deepayan137.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}